{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started with Darcy AI The Darcy AI SDK offers a rich set of features that Darcy AI application developers can use to easily build complex AI processing chains. This makes developing rich real-time applications possible in a shorter timeframe and with a much more standardized approach. If you have not yet read the overview of Darcy AI terminology, it is recommended that you do that first. You can find it here Darcy AI Terminology Guide Thinking in terms of Darcy AI pipelines The concept of an AI pipeline is similar to complex event processing (CEP) and data stream processing, but there are some unique aspects you will notice when building with the Darcy AI. You are allowed only one Darcy AI pipeline in your application. There is a reason for this. A pipeline allows you to order the AI operations in a way that produces predictable trade-offs. One example is placing two different AI operations in parallel. On a hardware system that does not have enough AI accelerator hardware, Darcy will need to make a decision about how to run those operations. If you had more than one pipeline, Darcy would have conflicting intelligence about how to sequence the operations and you would be unable to predict which pipeline would be able to process. You should consider a pipeline to be the backbone of your application flow. You only want one backbone and it should include all of the AI processing that you need in order to make your application run smoothly. The way you structure the pipeline will have an effect on AI processing speed and timing reliability. For processes that must occur in a straight line, attach processing steps called Perceptors one after the other. For processes that can take place in any order and do not depend on one another, you can use parallel ordering. A Darcy AI pipeline is a data graph and can be modeled visually like a sequence tree. p1 (0) p2 (1) p3 (1) / \\ | | p11 (0) p12 (1) p21 (0) p31 (1) | p121 (1) Full trips through the pipeline are your application \"main loop\" Every application has a main processing loop that defines the code structure. Everything is built around that main loop and often gets triggered as part of the main loop\u2019s operations. For your Darcy AI application, you should think of each full trip through the pipeline as one iteration of your main loop. This means you should include all of the AI processing steps that you want to happen every time in your pipeline. Processing steps that should only take place on certain conditions may be best implemented as immediate AI processing (see below) so Darcy does not use precious resources for processing that you don\u2019t want. Start with an Input Stream The first stage of every pipeline cycle (also called a frame or pulse) is the unprocessed data coming from the Input Stream that you have chosen for your application. Choose an Input Stream that provides the sensor data that you want Darcy to process. This may be audio, video, LiDAR, thermal video, or just about anything you can imagine. A good example of an Input Stream is the CameraStream class that comes built-in with the Darcy AI SDK. This Input Stream allows you to specify the device path for a video camera. It will read the video camera feed and bring it into Darcy at the frame rate and resolution you specify. Instantiate the CameraStream object and set some of its parameters like this: from darcyai.input.camera_stream import CameraStream camera = CameraStream(video_device=\"/dev/video0\", fps=20) Attach a Perceptor The main processing of the Darcy AI Pipeline is found in the Percpetors. Adding a Perceptor is easy. You just instantiate the Perceptor class, perform any initial operations to set it up, and then add it to the Pipeline in whatever position you desire. Each Perceptor offers different configuration options and produces different results. Perceptors also offer events to which you can subscribe. A good example of a powerful Perceptor is the People Perceptor that is built-in with the Darcy AI SDK. This Perceptor is focused on detecting and processing people so you, as the developer, can simply work with semantic data results. Here is an example of creating a People Perceptor instance and adding it to the Pipeline: from darcyai.perceptor.coral.people_perceptor import PeoplePerceptor people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_input_callback) Every pipeline step stores data in the Perception Object Model (POM) When a Perceptor has executed, its results are added to the Perception Object Model (POM) and the Pipeline continues to the next Perceptor or Output Stream if there are not further Perceptors in the Pipeline. The POM is like a shopping cart that gets loaded with data as it moves along. Everything is categorized in the POM so you can easily access the data associated with any Perceptor. The reference is the name you gave that Perceptor when adding it to the Pipeline. Every Perceptor produces its own specific data structure and may also provide convenience functions for performing operations on the result data, such as retrieving a particular person from a set or grabbing the face image of a specific person. The data structure and set of convenience functions for each Perceptor can be found in the documentation for that Perceptor. Here is an example of a callback that is taking advantage of several powerful convenience functions in the POM under the results of the People Perceptor named as \u201cmypeople\u201d: def my_callback(pom, input_data): current_display_frame = pom.mypeople.annotatedFrame().copy() all_people = pom.mypeople.people() current_number_of_people = pom.mypeople.peopleCount() Using Output Streams The last stage of every pipeline cycle is the set of Output Streams that you have added to the pipeline. Any number of Output Streams can be added, giving you the power to put data in many places or perform complex operations such as storing it locally, sending it upstream to a cloud environment, and also displaying a UI at the same time. Output Streams provide a callback which allows you, as the developer, to prepare the data that will be processed by the Output Stream. This is very useful if you want to format data before sending upstream, filter data before storing it on disk, or edit a video frame before you display it. A good example of an Output Stream is the LiveFeedStream class that comes built-in with the Darcy AI SDK. This Output Stream allows you to configure network host and port information and it will open a video feed that you can view with any web browser. Instantiate the LiveFeed output stream object and set some of its parameters like this: from darcyai.output.live_feed_stream import LiveFeedStream def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.mypeople.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} people\".format(pom.mypeople.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) The Perceptor input and output callbacks Every Perceptor that is added to the Pipeline provides both an input callback and output callback. The input callback is available for you, as the developer, to prepare the data that will be passed to the perceptor. The input callback signature is any function that accepts three parameters. The parameters are an input data object, POM object, and configuration object. Pass your input callback function as a parameter when you add the Perceptor to the Pipeline. def people_perceptor_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_perceptor_input_callback) The output callback is intended for you to perform filtering, refinement, editing, and any other data operations on the output of the Perceptor before Darcy moves further down the Pipeline. In many cases, you will not have a need to use this callback. The output of the Perceptor will be usable in its original form. One example of a good use of the output callback, though, is to remove data from the POM that does not fit certain business criteria, such as deleting people from the POM who are facing the wrong direction. Usage of the output callback is similar to the input callback. def people_perceptor_output_callback(perceptor_input, pom, config): #Just take the last person from the results all_people = pom.mypeople.people() filtered_data = None for person in all_people: filtered_data = person return filtered_data people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, output_callback=people_perceptor_output_callback) Configuring Perceptors and Output Streams There are two ways to configure the Perceptors and Output Streams in the Darcy AI system. One approach is to set the configuration in code. The other approach is to use the configuration REST API that becomes available when your Darcy AI application is running. Both approaches also provide the ability for you to fetch the current configuration of both Perceptors and Output Streams. To retrieve the current configuration of a Perceptor or Output Stream in code, call the correct method on the Pipeline object. perceptor_config_dictionary = pipeline.get_perceptor_config(\"mypeople\") outstream_config_dictionary = pipeline.get_output_stream_config(\"videoout\") To set a configuration item in code, call the pipeline method and pass the name of the configuration item as a string and also pass the new value. The list of configuration items will be provided in the Perceptor or Output Stream documentation, along with accepted types of values. pipeline.set_perceptor_config(\"mypeople\", \"show_pose_landmark_dots\", True) To retrieve the current configuration using the REST API, use the following URIs and fill in your device hostname or IP address and replace the Perceptor or Output Stream name with the name you have chosen when adding it to the pipeline. GET http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config GET http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config And to use the REST API to make changes, pass your updated configuration JSON to the following URIs as a PATCH request. PATCH http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config PATCH http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config Subscribing to Perceptor events Perceptors may provide events to which you can subscribe with a callback function. The list of events that a Perceptor provides will be listed in the documentation for that specific Perceptor. To subscribe to an event you pass a callback function with the proper number of parameters as listed in the documentation for that event. Use the .on() method of a Perceptor to subscribe and pass the event name as a string. def new_person_callback(person_id): print(person_id) people_ai = PeoplePerceptor() people_ai.on(\"new_person_entered_scene\", new_person_callback) Immediate and conditional AI processing While the Darcy AI Pipeline is intended for executing AI processing against all incoming data, there are times when you may want to run an AI process against arbitrary data immediately. You may also want to selectively run an AI process under certain conditions but not others. The Pipeline object provides a method for executing a Perceptor against arbitrary data. You can use any Perceptor. The data you pass must be in the form of a StreamData object, which means that you can put any data in the object but you must also add an integer timestamp. people_ai = PeoplePerceptor() current_time = int(time.time()) my_data = StreamData(saved_video_frame, current_time) pipeline.run_perceptor(people_ai, my_data) Bring it all together with a full application The best way to learn all of these concepts is to see them in action. Start with building a sample application that illustrates all of these topics in actual code. Then build your own! You can find a sample application guide here Build Guide .","title":"Home"},{"location":"#getting-started-with-darcy-ai","text":"The Darcy AI SDK offers a rich set of features that Darcy AI application developers can use to easily build complex AI processing chains. This makes developing rich real-time applications possible in a shorter timeframe and with a much more standardized approach. If you have not yet read the overview of Darcy AI terminology, it is recommended that you do that first. You can find it here Darcy AI Terminology Guide","title":"Getting started with Darcy AI"},{"location":"#thinking-in-terms-of-darcy-ai-pipelines","text":"The concept of an AI pipeline is similar to complex event processing (CEP) and data stream processing, but there are some unique aspects you will notice when building with the Darcy AI. You are allowed only one Darcy AI pipeline in your application. There is a reason for this. A pipeline allows you to order the AI operations in a way that produces predictable trade-offs. One example is placing two different AI operations in parallel. On a hardware system that does not have enough AI accelerator hardware, Darcy will need to make a decision about how to run those operations. If you had more than one pipeline, Darcy would have conflicting intelligence about how to sequence the operations and you would be unable to predict which pipeline would be able to process. You should consider a pipeline to be the backbone of your application flow. You only want one backbone and it should include all of the AI processing that you need in order to make your application run smoothly. The way you structure the pipeline will have an effect on AI processing speed and timing reliability. For processes that must occur in a straight line, attach processing steps called Perceptors one after the other. For processes that can take place in any order and do not depend on one another, you can use parallel ordering. A Darcy AI pipeline is a data graph and can be modeled visually like a sequence tree. p1 (0) p2 (1) p3 (1) / \\ | | p11 (0) p12 (1) p21 (0) p31 (1) | p121 (1)","title":"Thinking in terms of Darcy AI pipelines"},{"location":"#full-trips-through-the-pipeline-are-your-application-main-loop","text":"Every application has a main processing loop that defines the code structure. Everything is built around that main loop and often gets triggered as part of the main loop\u2019s operations. For your Darcy AI application, you should think of each full trip through the pipeline as one iteration of your main loop. This means you should include all of the AI processing steps that you want to happen every time in your pipeline. Processing steps that should only take place on certain conditions may be best implemented as immediate AI processing (see below) so Darcy does not use precious resources for processing that you don\u2019t want.","title":"Full trips through the pipeline are your application \"main loop\""},{"location":"#start-with-an-input-stream","text":"The first stage of every pipeline cycle (also called a frame or pulse) is the unprocessed data coming from the Input Stream that you have chosen for your application. Choose an Input Stream that provides the sensor data that you want Darcy to process. This may be audio, video, LiDAR, thermal video, or just about anything you can imagine. A good example of an Input Stream is the CameraStream class that comes built-in with the Darcy AI SDK. This Input Stream allows you to specify the device path for a video camera. It will read the video camera feed and bring it into Darcy at the frame rate and resolution you specify. Instantiate the CameraStream object and set some of its parameters like this: from darcyai.input.camera_stream import CameraStream camera = CameraStream(video_device=\"/dev/video0\", fps=20)","title":"Start with an Input Stream"},{"location":"#attach-a-perceptor","text":"The main processing of the Darcy AI Pipeline is found in the Percpetors. Adding a Perceptor is easy. You just instantiate the Perceptor class, perform any initial operations to set it up, and then add it to the Pipeline in whatever position you desire. Each Perceptor offers different configuration options and produces different results. Perceptors also offer events to which you can subscribe. A good example of a powerful Perceptor is the People Perceptor that is built-in with the Darcy AI SDK. This Perceptor is focused on detecting and processing people so you, as the developer, can simply work with semantic data results. Here is an example of creating a People Perceptor instance and adding it to the Pipeline: from darcyai.perceptor.coral.people_perceptor import PeoplePerceptor people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_input_callback)","title":"Attach a Perceptor"},{"location":"#every-pipeline-step-stores-data-in-the-perception-object-model-pom","text":"When a Perceptor has executed, its results are added to the Perception Object Model (POM) and the Pipeline continues to the next Perceptor or Output Stream if there are not further Perceptors in the Pipeline. The POM is like a shopping cart that gets loaded with data as it moves along. Everything is categorized in the POM so you can easily access the data associated with any Perceptor. The reference is the name you gave that Perceptor when adding it to the Pipeline. Every Perceptor produces its own specific data structure and may also provide convenience functions for performing operations on the result data, such as retrieving a particular person from a set or grabbing the face image of a specific person. The data structure and set of convenience functions for each Perceptor can be found in the documentation for that Perceptor. Here is an example of a callback that is taking advantage of several powerful convenience functions in the POM under the results of the People Perceptor named as \u201cmypeople\u201d: def my_callback(pom, input_data): current_display_frame = pom.mypeople.annotatedFrame().copy() all_people = pom.mypeople.people() current_number_of_people = pom.mypeople.peopleCount()","title":"Every pipeline step stores data in the Perception Object Model (POM)"},{"location":"#using-output-streams","text":"The last stage of every pipeline cycle is the set of Output Streams that you have added to the pipeline. Any number of Output Streams can be added, giving you the power to put data in many places or perform complex operations such as storing it locally, sending it upstream to a cloud environment, and also displaying a UI at the same time. Output Streams provide a callback which allows you, as the developer, to prepare the data that will be processed by the Output Stream. This is very useful if you want to format data before sending upstream, filter data before storing it on disk, or edit a video frame before you display it. A good example of an Output Stream is the LiveFeedStream class that comes built-in with the Darcy AI SDK. This Output Stream allows you to configure network host and port information and it will open a video feed that you can view with any web browser. Instantiate the LiveFeed output stream object and set some of its parameters like this: from darcyai.output.live_feed_stream import LiveFeedStream def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.mypeople.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} people\".format(pom.mypeople.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") pipeline.add_output_stream(\"output\", live_feed_callback, live_feed)","title":"Using Output Streams"},{"location":"#the-perceptor-input-and-output-callbacks","text":"Every Perceptor that is added to the Pipeline provides both an input callback and output callback. The input callback is available for you, as the developer, to prepare the data that will be passed to the perceptor. The input callback signature is any function that accepts three parameters. The parameters are an input data object, POM object, and configuration object. Pass your input callback function as a parameter when you add the Perceptor to the Pipeline. def people_perceptor_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, input_callback=people_perceptor_input_callback) The output callback is intended for you to perform filtering, refinement, editing, and any other data operations on the output of the Perceptor before Darcy moves further down the Pipeline. In many cases, you will not have a need to use this callback. The output of the Perceptor will be usable in its original form. One example of a good use of the output callback, though, is to remove data from the POM that does not fit certain business criteria, such as deleting people from the POM who are facing the wrong direction. Usage of the output callback is similar to the input callback. def people_perceptor_output_callback(perceptor_input, pom, config): #Just take the last person from the results all_people = pom.mypeople.people() filtered_data = None for person in all_people: filtered_data = person return filtered_data people_ai = PeoplePerceptor() pipeline.add_perceptor(\"mypeople\", people_ai, output_callback=people_perceptor_output_callback)","title":"The Perceptor input and output callbacks"},{"location":"#configuring-perceptors-and-output-streams","text":"There are two ways to configure the Perceptors and Output Streams in the Darcy AI system. One approach is to set the configuration in code. The other approach is to use the configuration REST API that becomes available when your Darcy AI application is running. Both approaches also provide the ability for you to fetch the current configuration of both Perceptors and Output Streams. To retrieve the current configuration of a Perceptor or Output Stream in code, call the correct method on the Pipeline object. perceptor_config_dictionary = pipeline.get_perceptor_config(\"mypeople\") outstream_config_dictionary = pipeline.get_output_stream_config(\"videoout\") To set a configuration item in code, call the pipeline method and pass the name of the configuration item as a string and also pass the new value. The list of configuration items will be provided in the Perceptor or Output Stream documentation, along with accepted types of values. pipeline.set_perceptor_config(\"mypeople\", \"show_pose_landmark_dots\", True) To retrieve the current configuration using the REST API, use the following URIs and fill in your device hostname or IP address and replace the Perceptor or Output Stream name with the name you have chosen when adding it to the pipeline. GET http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config GET http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config And to use the REST API to make changes, pass your updated configuration JSON to the following URIs as a PATCH request. PATCH http://HOSTNAME_OR_IP:8080/pipeline/perceptors/PERCEPTOR_NAME/config PATCH http://HOSTNAME_OR_IP:8080/pipeline/outputs/OUTPUT_STREAM/config","title":"Configuring Perceptors and Output Streams"},{"location":"#subscribing-to-perceptor-events","text":"Perceptors may provide events to which you can subscribe with a callback function. The list of events that a Perceptor provides will be listed in the documentation for that specific Perceptor. To subscribe to an event you pass a callback function with the proper number of parameters as listed in the documentation for that event. Use the .on() method of a Perceptor to subscribe and pass the event name as a string. def new_person_callback(person_id): print(person_id) people_ai = PeoplePerceptor() people_ai.on(\"new_person_entered_scene\", new_person_callback)","title":"Subscribing to Perceptor events"},{"location":"#immediate-and-conditional-ai-processing","text":"While the Darcy AI Pipeline is intended for executing AI processing against all incoming data, there are times when you may want to run an AI process against arbitrary data immediately. You may also want to selectively run an AI process under certain conditions but not others. The Pipeline object provides a method for executing a Perceptor against arbitrary data. You can use any Perceptor. The data you pass must be in the form of a StreamData object, which means that you can put any data in the object but you must also add an integer timestamp. people_ai = PeoplePerceptor() current_time = int(time.time()) my_data = StreamData(saved_video_frame, current_time) pipeline.run_perceptor(people_ai, my_data)","title":"Immediate and conditional AI processing"},{"location":"#bring-it-all-together-with-a-full-application","text":"The best way to learn all of these concepts is to see them in action. Start with building a sample application that illustrates all of these topics in actual code. Then build your own! You can find a sample application guide here Build Guide .","title":"Bring it all together with a full application"},{"location":"build/","text":"Building your Darcy AI application It\u2019s easy to build a Darcy AI application but how do you get started? Here\u2019s an example application that introduces all of the main concepts you will need for building your own application. Start by following along! Requirements You\u2019ll need to have a few things in place before you build. Here\u2019s the list: - Visual Studio Code (VS Code) with Python extensions - https://code.visualstudio.com/ - https://marketplace.visualstudio.com/items?itemName=ms-python.python - A Raspberry Pi with an attached video camera and Google Coral edge TPU - Set up your VS Code environment to develop on your Darcy Cam or Raspberry Pi remotely - https://www.raspberrypi.com/news/coding-on-raspberry-pi-remotely-with-visual-studio-code/ - Python 3.5+ - Docker on your Raspberry Pi Create your application Python file and import libraries You only need a single Python file to build a Darcy AI application. Open a new .py file in VS Code and name it whatever you want. Then add the following statements at the top to include the Darcy AI libraries and some additional helpful libraries: import cv2 import os import pathlib from darcyai.perceptor.coral.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline If you don\u2019t have the darcyai library installed yet, you can install it with PIP package installer for Python using the following commands, which you should run both on your development workstation and on your Raspberry Pi where you will be running your application: pip install darcyai Add the Pipeline, Input Stream, and Output Stream objects This part is quite easy. Just follow the comments to learn more about these 3 important lines of code. #Instantiate an Camera Stream input stream object camera = CameraStream(video_device=\"/dev/video0\", fps=20) #Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) #Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") Set up a callback and add the Output Stream to the Pipeline Before we add the LiveFeed Output Stream to the Pipeline, we need to set up a callback function that we are going to use to process the data before displaying the video. Follow the comments to learn about the steps that are taken. This is the most complex portion of the whole application and it is where all of the business logic is taking place. After the callback function definition, there is a line for adding the LiveFeed Output Stream to the Pipeline. That command needs to have the callback function already defined before it can execute successfully. #Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #If we have anyone, demonstrate looking up that person in the POM by getting their face size #And then put it on the frame as some text #NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame #Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) Define an event callback and an input callback and instantiate the People Perceptor Just like the LiveFeed Output Stream, the People Perceptor must have the callbacks already defined before it can work with those callbacks. The input callback simply takes the Input Stream data and sends it onward to the People Perceptor. The \u201cNew Person\u201d event callback simply prints the unique person identifier string to the console output when a new person has been detected by Darcy. #Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame #Create a callback function for handling the \"New Person\" event from the People Perceptor #Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) #Instantiate a People Perceptor people_ai = PeoplePerceptor() #Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback) Add the People Perceptor to the Pipeline #Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback) Change some configuration items in the People Perceptor #Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", \"0,255,0\") Start the Pipeline #Start the Pipeline pipeline.run() Check your completed code Your finished Python file should look similar to this. If it doesn\u2019t, take a minute to figure out what is missing or incorrect. Next we will build an application container from this code. import cv2 import os import pathlib from darcyai.perceptor.coral.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline #Instantiate an Camera Stream input stream object camera = CameraStream(video_device=\"/dev/video0\", fps=20) #Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) #Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") #Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #If we have anyone, demonstrate looking up that person in the POM by getting their face size #And then put it on the frame as some text #NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame #Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) #Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame #Create a callback function for handling the \"New Person\" event from the People Perceptor #Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) #Instantiate a People Perceptor people_ai = PeoplePerceptor() #Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback) #Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback) #Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", \"0,255,0\") #Start the Pipeline pipeline.run() Save your Python file to your Raspberry Pi If you are using VS Code remote development, then your file should automatically save on the device when you save in VS Code. If you are manually adding your file to your Raspberry Pi, copy the file to the device now. Add a Dockerfile to the same directory as your Python file You will build a Docker container to run your Darcy AI application. You only need your Python file and a Dockerfile to build the container. Make sure you create this Dockerfile in the same directory as your Python file and change the name from YOURFILE.py to the actual name of your file. FROM darcyai/darcy-ai-coral:dev RUN python3 -m pip install --upgrade darcyai COPY YOURFILE.py /src/my_app.py ENTRYPOINT [\"/bin/bash\", \"-c\", \"cd /src/ && python3 -u ./my_app.py\"] Build your Docker container Use the following command to build your Docker container. It may take 10 or 15 minutes if you are building for the first time and you do not have a very fast internet connection. This is because the underlying container base images will need to be downloaded. After the first build, this process should only take a minute or two. sudo docker build -t darcydev/my-people-ai-app:1.0.0 . Run your application Use this Docker command to run your application container right away. You can also use this Docker container with the Darcy Cloud to deploy and manage the application. sudo docker run -d --privileged -p 3456:3456 -p 8080:8080 -v /dev:/dev darcydev/my-people-ai-app:1.0.0 View your real-time Darcy AI application video output Once your application container is running, you can view the live video feed by visiting the following URL in any browser. Replace YOUR.DEVICE.IP.ADDRESS with the actual IP address of your Raspberry Pi. https://YOUR.DEVICE.IP.ADDRESS:3456/","title":"Build"},{"location":"build/#building-your-darcy-ai-application","text":"It\u2019s easy to build a Darcy AI application but how do you get started? Here\u2019s an example application that introduces all of the main concepts you will need for building your own application. Start by following along!","title":"Building your Darcy AI application"},{"location":"build/#requirements","text":"You\u2019ll need to have a few things in place before you build. Here\u2019s the list: - Visual Studio Code (VS Code) with Python extensions - https://code.visualstudio.com/ - https://marketplace.visualstudio.com/items?itemName=ms-python.python - A Raspberry Pi with an attached video camera and Google Coral edge TPU - Set up your VS Code environment to develop on your Darcy Cam or Raspberry Pi remotely - https://www.raspberrypi.com/news/coding-on-raspberry-pi-remotely-with-visual-studio-code/ - Python 3.5+ - Docker on your Raspberry Pi","title":"Requirements"},{"location":"build/#create-your-application-python-file-and-import-libraries","text":"You only need a single Python file to build a Darcy AI application. Open a new .py file in VS Code and name it whatever you want. Then add the following statements at the top to include the Darcy AI libraries and some additional helpful libraries: import cv2 import os import pathlib from darcyai.perceptor.coral.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline If you don\u2019t have the darcyai library installed yet, you can install it with PIP package installer for Python using the following commands, which you should run both on your development workstation and on your Raspberry Pi where you will be running your application: pip install darcyai","title":"Create your application Python file and import libraries"},{"location":"build/#add-the-pipeline-input-stream-and-output-stream-objects","text":"This part is quite easy. Just follow the comments to learn more about these 3 important lines of code. #Instantiate an Camera Stream input stream object camera = CameraStream(video_device=\"/dev/video0\", fps=20) #Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) #Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\")","title":"Add the Pipeline, Input Stream, and Output Stream objects"},{"location":"build/#set-up-a-callback-and-add-the-output-stream-to-the-pipeline","text":"Before we add the LiveFeed Output Stream to the Pipeline, we need to set up a callback function that we are going to use to process the data before displaying the video. Follow the comments to learn about the steps that are taken. This is the most complex portion of the whole application and it is where all of the business logic is taking place. After the callback function definition, there is a line for adding the LiveFeed Output Stream to the Pipeline. That command needs to have the callback function already defined before it can execute successfully. #Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #If we have anyone, demonstrate looking up that person in the POM by getting their face size #And then put it on the frame as some text #NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame #Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed)","title":"Set up a callback and add the Output Stream to the Pipeline"},{"location":"build/#define-an-event-callback-and-an-input-callback-and-instantiate-the-people-perceptor","text":"Just like the LiveFeed Output Stream, the People Perceptor must have the callbacks already defined before it can work with those callbacks. The input callback simply takes the Input Stream data and sends it onward to the People Perceptor. The \u201cNew Person\u201d event callback simply prints the unique person identifier string to the console output when a new person has been detected by Darcy. #Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame #Create a callback function for handling the \"New Person\" event from the People Perceptor #Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) #Instantiate a People Perceptor people_ai = PeoplePerceptor() #Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback)","title":"Define an event callback and an input callback and instantiate the People Perceptor"},{"location":"build/#add-the-people-perceptor-to-the-pipeline","text":"#Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback)","title":"Add the People Perceptor to the Pipeline"},{"location":"build/#change-some-configuration-items-in-the-people-perceptor","text":"#Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", \"0,255,0\")","title":"Change some configuration items in the People Perceptor"},{"location":"build/#start-the-pipeline","text":"#Start the Pipeline pipeline.run()","title":"Start the Pipeline"},{"location":"build/#check-your-completed-code","text":"Your finished Python file should look similar to this. If it doesn\u2019t, take a minute to figure out what is missing or incorrect. Next we will build an application container from this code. import cv2 import os import pathlib from darcyai.perceptor.coral.people_perceptor import PeoplePerceptor from darcyai.input.camera_stream import CameraStream from darcyai.output.live_feed_stream import LiveFeedStream from darcyai.pipeline import Pipeline #Instantiate an Camera Stream input stream object camera = CameraStream(video_device=\"/dev/video0\", fps=20) #Instantiate the Pipeline object and pass it the Camera Stream object as its input stream source pipeline = Pipeline(input_stream=camera) #Create a Live Feed output stream object and specify some URL parameters live_feed = LiveFeedStream(path=\"/\", port=3456, host=\"0.0.0.0\") #Create a callback function for handling the Live Feed output stream data before it gets presented def live_feed_callback(pom, input_data): #Start wth the annotated video frame available from the People Perceptor frame = pom.peeps.annotatedFrame().copy() #Add some text telling how many people are in the scene label = \"{} peeps\".format(pom.peeps.peopleCount()) color = (0, 255, 0) cv2.putText(frame, str(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #If we have anyone, demonstrate looking up that person in the POM by getting their face size #And then put it on the frame as some text #NOTE: this will just take the face size from the last person in the array if pom.peeps.peopleCount() > 0: for person_id in pom.peeps.people(): face_size = pom.peeps.faceSize(person_id) face_height = face_size[1] label2 = \"{} face height\".format(face_height) color = (0, 255, 255) cv2.putText(frame, str(label2), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA) #Pass the finished frame out of this callback so the Live Feed output stream can display it return frame #Add the Live Feed output stream to the Pipeline and use the callback from above as the handler pipeline.add_output_stream(\"output\", live_feed_callback, live_feed) #Create a callback function for handling the input that is about to pass to the People Perceptor def people_input_callback(input_data, pom, config): #Just take the frame from the incoming Input Stream and send it onward - no need to modify the frame frame = input_data.data.copy() return frame #Create a callback function for handling the \"New Person\" event from the People Perceptor #Just print the person ID to the console def new_person_callback(person_id): print(\"New person: {}\".format(person_id)) #Instantiate a People Perceptor people_ai = PeoplePerceptor() #Subscribe to the \"New Person\" event from the People Perceptor and use our callback from above as the handler people_ai.on(\"new_person_entered_scene\", new_person_callback) #Add the People Perceptor instance to the Pipeline and use the input callback from above as the input preparation handler pipeline.add_perceptor(\"peeps\", people_ai, input_callback=people_input_callback) #Update the configuration of the People Perceptor to show the pose landmark dots on the annotated video frame pipeline.set_perceptor_config(\"peeps\", \"show_pose_landmark_dots\", True) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_size\", 2) pipeline.set_perceptor_config(\"peeps\", \"pose_landmark_dot_color\", \"0,255,0\") #Start the Pipeline pipeline.run()","title":"Check your completed code"},{"location":"build/#save-your-python-file-to-your-raspberry-pi","text":"If you are using VS Code remote development, then your file should automatically save on the device when you save in VS Code. If you are manually adding your file to your Raspberry Pi, copy the file to the device now.","title":"Save your Python file to your Raspberry Pi"},{"location":"build/#add-a-dockerfile-to-the-same-directory-as-your-python-file","text":"You will build a Docker container to run your Darcy AI application. You only need your Python file and a Dockerfile to build the container. Make sure you create this Dockerfile in the same directory as your Python file and change the name from YOURFILE.py to the actual name of your file. FROM darcyai/darcy-ai-coral:dev RUN python3 -m pip install --upgrade darcyai COPY YOURFILE.py /src/my_app.py ENTRYPOINT [\"/bin/bash\", \"-c\", \"cd /src/ && python3 -u ./my_app.py\"]","title":"Add a Dockerfile to the same directory as your Python file"},{"location":"build/#build-your-docker-container","text":"Use the following command to build your Docker container. It may take 10 or 15 minutes if you are building for the first time and you do not have a very fast internet connection. This is because the underlying container base images will need to be downloaded. After the first build, this process should only take a minute or two. sudo docker build -t darcydev/my-people-ai-app:1.0.0 .","title":"Build your Docker container"},{"location":"build/#run-your-application","text":"Use this Docker command to run your application container right away. You can also use this Docker container with the Darcy Cloud to deploy and manage the application. sudo docker run -d --privileged -p 3456:3456 -p 8080:8080 -v /dev:/dev darcydev/my-people-ai-app:1.0.0","title":"Run your application"},{"location":"build/#view-your-real-time-darcy-ai-application-video-output","text":"Once your application container is running, you can view the live video feed by visiting the following URL in any browser. Replace YOUR.DEVICE.IP.ADDRESS with the actual IP address of your Raspberry Pi. https://YOUR.DEVICE.IP.ADDRESS:3456/","title":"View your real-time Darcy AI application video output"},{"location":"config/","text":"darcyai.config Config Objects class Config() Class to hold the configuration for the Perceptor. Arguments name ( str ): The name of the config. config_type ( str ): The type of the config. Valid types are: int float bool str default_value ( Any ): The default value of the config. description ( str ): The description of the config. is_valid def is_valid(value: Any) -> bool Checks if the value is valid for the config. Arguments value ( Any ): The value to check. Returns bool : True if the value is valid, False otherwise. cast def cast(value: Any) -> Any Casts the value to the type of the config. Arguments value ( Any ): The value to cast. Returns Any : The casted value.","title":"Config"},{"location":"config/#darcyaiconfig","text":"","title":"darcyai.config"},{"location":"config/#config-objects","text":"class Config() Class to hold the configuration for the Perceptor. Arguments name ( str ): The name of the config. config_type ( str ): The type of the config. Valid types are: int float bool str default_value ( Any ): The default value of the config. description ( str ): The description of the config.","title":"Config Objects"},{"location":"config/#is_valid","text":"def is_valid(value: Any) -> bool Checks if the value is valid for the config. Arguments value ( Any ): The value to check. Returns bool : True if the value is valid, False otherwise.","title":"is_valid"},{"location":"config/#cast","text":"def cast(value: Any) -> Any Casts the value to the type of the config. Arguments value ( Any ): The value to cast. Returns Any : The casted value.","title":"cast"},{"location":"perceptionobjectmodel/","text":"darcyai.perception_object_model PerceptionObjectModel Objects class PerceptionObjectModel(Serializable) This class is used to represent the perception of an object. set_value def set_value(key: str, value: Any) -> None Set the value of a key in the perception object model. Arguments key ( str ): The key to set. value ( Any ): The value to set. get_perceptor def get_perceptor(name: str) -> Any Get the perception result of the provided perceptor. Arguments name ( str ): The name of the perceptor. Returns Any : The result of the perception. get_perceptors def get_perceptors() -> List[str] Returns list of the perceptors. Returns List[str] : The list of the perceptors. serialize def serialize() -> Dict[str, Any] Serialize the perception object model. Returns Dict[str, Any] - The serialized perception object model. set_input_data def set_input_data(input_data: StreamData) -> None Set the input data for the perception object model. Arguments input_data ( StreamData ): The input data. get_input_data def get_input_data() -> StreamData Get the input data for the perception object model. Returns StreamData : The input data. set_pulse_number def set_pulse_number(pulse_number: int) -> None Set the pulse number for the perception object model. Arguments pulse_number ( int ): The pulse number. get_pulse_number def get_pulse_number() -> int Get the pulse number for the perception object model. Returns int : The pulse number.","title":"PerceptionObjectModel"},{"location":"perceptionobjectmodel/#darcyaiperception_object_model","text":"","title":"darcyai.perception_object_model"},{"location":"perceptionobjectmodel/#perceptionobjectmodel-objects","text":"class PerceptionObjectModel(Serializable) This class is used to represent the perception of an object.","title":"PerceptionObjectModel Objects"},{"location":"perceptionobjectmodel/#set_value","text":"def set_value(key: str, value: Any) -> None Set the value of a key in the perception object model. Arguments key ( str ): The key to set. value ( Any ): The value to set.","title":"set_value"},{"location":"perceptionobjectmodel/#get_perceptor","text":"def get_perceptor(name: str) -> Any Get the perception result of the provided perceptor. Arguments name ( str ): The name of the perceptor. Returns Any : The result of the perception.","title":"get_perceptor"},{"location":"perceptionobjectmodel/#get_perceptors","text":"def get_perceptors() -> List[str] Returns list of the perceptors. Returns List[str] : The list of the perceptors.","title":"get_perceptors"},{"location":"perceptionobjectmodel/#serialize","text":"def serialize() -> Dict[str, Any] Serialize the perception object model. Returns Dict[str, Any] - The serialized perception object model.","title":"serialize"},{"location":"perceptionobjectmodel/#set_input_data","text":"def set_input_data(input_data: StreamData) -> None Set the input data for the perception object model. Arguments input_data ( StreamData ): The input data.","title":"set_input_data"},{"location":"perceptionobjectmodel/#get_input_data","text":"def get_input_data() -> StreamData Get the input data for the perception object model. Returns StreamData : The input data.","title":"get_input_data"},{"location":"perceptionobjectmodel/#set_pulse_number","text":"def set_pulse_number(pulse_number: int) -> None Set the pulse number for the perception object model. Arguments pulse_number ( int ): The pulse number.","title":"set_pulse_number"},{"location":"perceptionobjectmodel/#get_pulse_number","text":"def get_pulse_number() -> int Get the pulse number for the perception object model. Returns int : The pulse number.","title":"get_pulse_number"},{"location":"perceptor/","text":"darcyai.perceptor.perceptor Perceptor Objects class Perceptor(Configurable, EventEmitter) The Perceptor class is the base class for all perceptors. Arguments model_path ( str ): The path to the model file. Examples >>> from darcyai.perceptor import Perceptor >>> class MyPerceptor(Perceptor): >>> def __init__(self): ... Perceptor.__init__(self, \"path/to/model\") >>> def run(self, input_data): ... return input_data.data >>> def load(self): ... pass run def run(input_data: Any, config: ConfigRegistry = None) -> Any Runs the perceptor on the input data. Arguments input_data ( StreamData ): The input data to run the perceptor on. config ( ConfigRegistry ): The configuration for the perceptor. Defaults to None . Returns Any : The output of the perceptor. load def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None . is_loaded def is_loaded() -> bool Checks if the perceptor is loaded. Returns bool : True if the perceptor is loaded, False otherwise. set_loaded def set_loaded(loaded: bool) -> None Sets the perceptor loaded state. Arguments loaded ( bool ): The loaded state. set_config_value def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set. get_config_value def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config. init_config_registry def init_config_registry() Initializes the config registry.","title":"Perceptor"},{"location":"perceptor/#darcyaiperceptorperceptor","text":"","title":"darcyai.perceptor.perceptor"},{"location":"perceptor/#perceptor-objects","text":"class Perceptor(Configurable, EventEmitter) The Perceptor class is the base class for all perceptors. Arguments model_path ( str ): The path to the model file. Examples >>> from darcyai.perceptor import Perceptor >>> class MyPerceptor(Perceptor): >>> def __init__(self): ... Perceptor.__init__(self, \"path/to/model\") >>> def run(self, input_data): ... return input_data.data >>> def load(self): ... pass","title":"Perceptor Objects"},{"location":"perceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> Any Runs the perceptor on the input data. Arguments input_data ( StreamData ): The input data to run the perceptor on. config ( ConfigRegistry ): The configuration for the perceptor. Defaults to None . Returns Any : The output of the perceptor.","title":"run"},{"location":"perceptor/#load","text":"def load(accelerator_idx: Union[int, None] = None) -> None Loads the perceptor. Arguments accelerator_idx ( int, None ): The index of the accelerator to load the perceptor on. Defaults to None .","title":"load"},{"location":"perceptor/#is_loaded","text":"def is_loaded() -> bool Checks if the perceptor is loaded. Returns bool : True if the perceptor is loaded, False otherwise.","title":"is_loaded"},{"location":"perceptor/#set_loaded","text":"def set_loaded(loaded: bool) -> None Sets the perceptor loaded state. Arguments loaded ( bool ): The loaded state.","title":"set_loaded"},{"location":"perceptor/#set_config_value","text":"def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set.","title":"set_config_value"},{"location":"perceptor/#get_config_value","text":"def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config.","title":"get_config_value"},{"location":"perceptor/#init_config_registry","text":"def init_config_registry() Initializes the config registry.","title":"init_config_registry"},{"location":"pipeline/","text":"darcyai.pipeline Pipeline Objects class Pipeline() The Pipeline class is the main class of the darcyai package. Arguments input_stream ( InputStream ): The input stream to be used by the pipeline. input_data_history_len ( int ): The number of input data items to be stored in the history. Defaults to 1 . pom_history_len ( int ): The number of POM items to be stored in the history. Defaults to 1 . metrics_history_len ( int ): The number of metrics items to be stored in the history. Defaults to 1 . num_of_edge_tpus ( int ): The number of Edge TPUs. Defaults to 1 . perceptor_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when a Perceptor throws an exception. Defaults to None . output_stream_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when an OutputStream throws an exception. Defaults to None . input_stream_error_handler_callback ( Callable[[Exception], None] ): The callback function to be called when an InputStream throws an exception. Defaults to None . perception_completion_callback ( Callable[[PerceptionObjectModel], None] ): The callback function to be called when all the perceptors have completed processing. Defaults to None . universal_rest_api ( bool ): Whether or not to use the universal REST API. Defaults to False . rest_api_base_path ( str ): The base path of the REST API. Defaults to / . rest_api_flask_app ( Flask ): The Flask application to be used by the REST API. Defaults to None . rest_api_port ( int ): The port of the REST API. Defaults to 5000 . rest_api_host ( str ): The host of the REST API. Defaults to localhost . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera, ... input_data_history_len=10, ... pom_history_len=10, ... metrics_history_len=10, ... num_of_edge_tpus=1, ... perceptor_error_handler_callback=None, ... output_stream_error_handler_callback=None, ... input_stream_error_handler_callback=None, ... perception_completion_callback=None, ... pulse_completion_callback=None, ... universal_rest_api=True, ... rest_api_base_path=\"/\", ... rest_api_flask_app=None, ... rest_api_port=5000, ... rest_api_host=\"localhost\") num_of_edge_tpus def num_of_edge_tpus() -> int Gets the number of Edge TPUs in the pipeline. Returns int : The number of Edge TPUs in the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.num_of_edge_tpus() add_perceptor def add_perceptor(name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[ [Any, PerceptionObjectModel, ConfigRegistry], Any] = None, parent: str = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: Dict[str, Any] = None) -> None Adds a new Perceptor to the pipeline. Arguments name ( str ): The name of the Perceptor (must be a valid variable name). perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . parent ( str ): The name of the parent Perceptor. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( Dict[str, Any] ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor(name=\"perceptor\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... parent=\"input_stream\", ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_perceptor_before def add_perceptor_before( name_to_insert_before: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_before ( str ): The name of the Perceptor to insert the new Perceptor before. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_before(name=\"perceptor\", ... name_to_insert_before=\"child_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_perceptor_after def add_perceptor_after( name_to_insert_after: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_after ( str ): The name of the Perceptor to insert the new Perceptor after. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, Any], ConfigRegistry] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_after(name=\"perceptor\", ... name_to_insert_after=\"parent_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) add_parallel_perceptor def add_parallel_perceptor( name_to_insert_in_parallel_with: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_in_parallel_with ( str ): The name of the Perceptor to insert the new Perceptor in parallel with. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_parallel_perceptor(name=\"perceptor\", ... name_to_insert_in_parallel_with=\"parallel_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"}) update_input_stream def update_input_stream(input_stream: InputStream) -> None Updates the input stream of the pipeline. Arguments input_stream ( InputStream ): The input stream to be added. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.update_input_stream(camera) <a id=\"darcyai.pipeline.Pipeline.add_output_stream\"></a> #### add\\_output\\_stream ```python def add_output_stream(name: str, callback: Callable[[PerceptionObjectModel, StreamData], Any], output_stream: OutputStream, default_config: dict = None) -> None Adds an OutputStream to the pipeline. Arguments name ( str ): The name of the OutputStream. callback ( Callable[[PerceptionObjectModel, StreamData], Any] ): A callback function that is called whith PerceptionObjectModel object and returns the data that the output stream must process. output_stream ( OutputStream ): The OutputStream to be added. default_config ( dict ): The default configuration for the OutputStream. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) remove_output_stream def remove_output_stream(name: str) -> None Removes an OutputStream from the pipeline. Arguments name ( str ): The name of the OutputStream to be removed. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera)\\ >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) >>> pipeline.remove_output_stream(name=\"output_stream\") stop def stop() -> None Stops the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.stop() run def run() -> None Runs the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.run() get_pom def get_pom() -> PerceptionObjectModel Gets the Perception Object Model. Returns PerceptionObjectModel : The Perception Object Model. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom = pipeline.get_pom() get_current_pulse_number def get_current_pulse_number() -> int Gets the current pulse number. Returns int : The current pulse number. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pulse_number = pipeline.get_current_pulse_number() get_latest_input def get_latest_input() -> StreamData Gets the latest input data. Returns StreamData : The latest input data. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> latest_input = pipeline.get_latest_input() get_historical_input def get_historical_input(pulse_number: int) -> StreamData Gets the input data from the history. Arguments pulse_number ( int ): The pulse number. Returns StreamData : The input data from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_input = pipeline.get_historical_input(pulse_number=1) get_input_history def get_input_history() -> Dict[int, StreamData] Gets the input data history. Returns Dict[int, StreamData] - The input data history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> input_history = pipeline.get_input_history() get_historical_pom def get_historical_pom(pulse_number: int) -> PerceptionObjectModel Gets the POM from the history. Arguments pulse_number ( int ): The pulse number. Returns PerceptionObjectModel : The POM from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_pom = pipeline.get_historical_pom(pulse_number=1) get_pom_history def get_pom_history() -> Dict[int, PerceptionObjectModel] Gets the POM history. Returns Dict[int, PerceptionObjectModel] - The POM history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom_history = pipeline.get_pom_history() run_perceptor def run_perceptor(perceptor: Perceptor, input_data: Any, multi: bool = False) -> Any Runs the Perceptor. Arguments perceptor ( Perceptor ): The Perceptor to be run. input_data ( Any ): The input data. multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . Returns Any : The result of running the Perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> result = pipeline.run_perceptor(perceptor=Perceptor(), input_data=None, multi=True) get_graph def get_graph() -> Any Gets the graph of the perceptors. Returns Any : The graph of the perceptors. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> graph = pipeline.get_graph() get_all_performance_metrics def get_all_performance_metrics() -> Dict[str, Any] Gets the performance metrics of the pipeline. Returns Dict[str, Any] - The performance metrics of the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_all_performance_metrics() get_pulse_performance_metrics def get_pulse_performance_metrics( pulse_number: Union[int, None] = None) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific pulse. Arguments pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific pulse. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_pulse_performance_metrics(pulse_number=1) get_perceptor_performance_metrics def get_perceptor_performance_metrics(name: str, pulse_number: Union[int, None] = None ) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific perceptor. Arguments name ( str ): The name of the perceptor. pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_perceptor_performance_metrics(name=\"perceptor_name\", ... pulse_number=1) set_perceptor_config def set_perceptor_config(perceptor_name: str, name: str, value: Any) -> None Sets the config of the pipeline. Arguments perceptor_name ( str ): The name of the perceptor. name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_perceptor_config(perceptor_name=\"perceptor_name\", ... name=\"config_name\", ... value=1) get_perceptor_config def get_perceptor_config(perceptor_name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the perceptor. Arguments perceptor_name ( str ): The name of the perceptor. Returns Dict[str, Tuple[Any, Config]] - The config of the perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_perceptor_config(perceptor_name=\"perceptor_name\") set_output_stream_config def set_output_stream_config(name: str, config_name: str, value: Any) -> None Sets the config of the output stream. Arguments name ( str ): The name of the output stream. config_name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_output_stream_config(name=\"output_stream_name\", ... config_name=\"config_name\", ... value=1) get_output_stream_config def get_output_stream_config(name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the output stream. Arguments name ( str ): The name of the output stream. Returns Dict[str, Tuple[Any, Config]] - The config of the output stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_output_stream_config(name=\"output_stream_name\")","title":"Pipeline"},{"location":"pipeline/#darcyaipipeline","text":"","title":"darcyai.pipeline"},{"location":"pipeline/#pipeline-objects","text":"class Pipeline() The Pipeline class is the main class of the darcyai package. Arguments input_stream ( InputStream ): The input stream to be used by the pipeline. input_data_history_len ( int ): The number of input data items to be stored in the history. Defaults to 1 . pom_history_len ( int ): The number of POM items to be stored in the history. Defaults to 1 . metrics_history_len ( int ): The number of metrics items to be stored in the history. Defaults to 1 . num_of_edge_tpus ( int ): The number of Edge TPUs. Defaults to 1 . perceptor_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when a Perceptor throws an exception. Defaults to None . output_stream_error_handler_callback ( Callable[[str, Exception], None] ): The callback function to be called when an OutputStream throws an exception. Defaults to None . input_stream_error_handler_callback ( Callable[[Exception], None] ): The callback function to be called when an InputStream throws an exception. Defaults to None . perception_completion_callback ( Callable[[PerceptionObjectModel], None] ): The callback function to be called when all the perceptors have completed processing. Defaults to None . universal_rest_api ( bool ): Whether or not to use the universal REST API. Defaults to False . rest_api_base_path ( str ): The base path of the REST API. Defaults to / . rest_api_flask_app ( Flask ): The Flask application to be used by the REST API. Defaults to None . rest_api_port ( int ): The port of the REST API. Defaults to 5000 . rest_api_host ( str ): The host of the REST API. Defaults to localhost . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera, ... input_data_history_len=10, ... pom_history_len=10, ... metrics_history_len=10, ... num_of_edge_tpus=1, ... perceptor_error_handler_callback=None, ... output_stream_error_handler_callback=None, ... input_stream_error_handler_callback=None, ... perception_completion_callback=None, ... pulse_completion_callback=None, ... universal_rest_api=True, ... rest_api_base_path=\"/\", ... rest_api_flask_app=None, ... rest_api_port=5000, ... rest_api_host=\"localhost\")","title":"Pipeline Objects"},{"location":"pipeline/#num_of_edge_tpus","text":"def num_of_edge_tpus() -> int Gets the number of Edge TPUs in the pipeline. Returns int : The number of Edge TPUs in the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.num_of_edge_tpus()","title":"num_of_edge_tpus"},{"location":"pipeline/#add_perceptor","text":"def add_perceptor(name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[ [Any, PerceptionObjectModel, ConfigRegistry], Any] = None, parent: str = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: Dict[str, Any] = None) -> None Adds a new Perceptor to the pipeline. Arguments name ( str ): The name of the Perceptor (must be a valid variable name). perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . parent ( str ): The name of the parent Perceptor. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( Dict[str, Any] ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor(name=\"perceptor\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... parent=\"input_stream\", ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor"},{"location":"pipeline/#add_perceptor_before","text":"def add_perceptor_before( name_to_insert_before: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_before ( str ): The name of the Perceptor to insert the new Perceptor before. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_before(name=\"perceptor\", ... name_to_insert_before=\"child_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor_before"},{"location":"pipeline/#add_perceptor_after","text":"def add_perceptor_after( name_to_insert_after: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_after ( str ): The name of the Perceptor to insert the new Perceptor after. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, Any], ConfigRegistry] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_perceptor_after(name=\"perceptor\", ... name_to_insert_after=\"parent_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_perceptor_after"},{"location":"pipeline/#add_parallel_perceptor","text":"def add_parallel_perceptor( name_to_insert_in_parallel_with: str, name: str, perceptor: Perceptor, input_callback: Callable[ [StreamData, PerceptionObjectModel, ConfigRegistry], Any] = None, output_callback: Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] = None, multi: bool = False, accelerator_idx: Union[int, None] = None, default_config: dict = None) -> None Adds a new Perceptor to the pipeline. Arguments name_to_insert_in_parallel_with ( str ): The name of the Perceptor to insert the new Perceptor in parallel with. name ( str ): The name of the Perceptor. perceptor ( Perceptor ): The Perceptor to be added. input_callback ( Callable[[StreamData, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor receives input data. Defaults to None . output_callback ( Callable[[Any, PerceptionObjectModel, ConfigRegistry], Any] ): The callback function to be called when the Perceptor produces output data. Defaults to None . multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . accelerator_idx ( int ): The index of the Edge TPU to be used by the Perceptor. Defaults to None . default_config ( dict ): The default configuration for the Perceptor. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_parallel_perceptor(name=\"perceptor\", ... name_to_insert_in_parallel_with=\"parallel_input_stream\", ... perceptor=MyPerceptor(), ... input_callback=None, ... output_callback=None, ... multi=True, ... accelerator_idx=0, ... default_config={\"key\": \"value\"})","title":"add_parallel_perceptor"},{"location":"pipeline/#update_input_stream","text":"def update_input_stream(input_stream: InputStream) -> None Updates the input stream of the pipeline. Arguments input_stream ( InputStream ): The input stream to be added. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.update_input_stream(camera) <a id=\"darcyai.pipeline.Pipeline.add_output_stream\"></a> #### add\\_output\\_stream ```python def add_output_stream(name: str, callback: Callable[[PerceptionObjectModel, StreamData], Any], output_stream: OutputStream, default_config: dict = None) -> None Adds an OutputStream to the pipeline. Arguments name ( str ): The name of the OutputStream. callback ( Callable[[PerceptionObjectModel, StreamData], Any] ): A callback function that is called whith PerceptionObjectModel object and returns the data that the output stream must process. output_stream ( OutputStream ): The OutputStream to be added. default_config ( dict ): The default configuration for the OutputStream. Defaults to None . Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"})","title":"update_input_stream"},{"location":"pipeline/#remove_output_stream","text":"def remove_output_stream(name: str) -> None Removes an OutputStream from the pipeline. Arguments name ( str ): The name of the OutputStream to be removed. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera)\\ >>> pipeline.add_output_stream(name=\"output_stream\", ... callback=None, ... output_stream=MyOutputStream(), ... default_config={\"key\": \"value\"}) >>> pipeline.remove_output_stream(name=\"output_stream\")","title":"remove_output_stream"},{"location":"pipeline/#stop","text":"def stop() -> None Stops the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.stop()","title":"stop"},{"location":"pipeline/#run","text":"def run() -> None Runs the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.run()","title":"run"},{"location":"pipeline/#get_pom","text":"def get_pom() -> PerceptionObjectModel Gets the Perception Object Model. Returns PerceptionObjectModel : The Perception Object Model. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom = pipeline.get_pom()","title":"get_pom"},{"location":"pipeline/#get_current_pulse_number","text":"def get_current_pulse_number() -> int Gets the current pulse number. Returns int : The current pulse number. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pulse_number = pipeline.get_current_pulse_number()","title":"get_current_pulse_number"},{"location":"pipeline/#get_latest_input","text":"def get_latest_input() -> StreamData Gets the latest input data. Returns StreamData : The latest input data. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> latest_input = pipeline.get_latest_input()","title":"get_latest_input"},{"location":"pipeline/#get_historical_input","text":"def get_historical_input(pulse_number: int) -> StreamData Gets the input data from the history. Arguments pulse_number ( int ): The pulse number. Returns StreamData : The input data from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_input = pipeline.get_historical_input(pulse_number=1)","title":"get_historical_input"},{"location":"pipeline/#get_input_history","text":"def get_input_history() -> Dict[int, StreamData] Gets the input data history. Returns Dict[int, StreamData] - The input data history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> input_history = pipeline.get_input_history()","title":"get_input_history"},{"location":"pipeline/#get_historical_pom","text":"def get_historical_pom(pulse_number: int) -> PerceptionObjectModel Gets the POM from the history. Arguments pulse_number ( int ): The pulse number. Returns PerceptionObjectModel : The POM from the history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> historical_pom = pipeline.get_historical_pom(pulse_number=1)","title":"get_historical_pom"},{"location":"pipeline/#get_pom_history","text":"def get_pom_history() -> Dict[int, PerceptionObjectModel] Gets the POM history. Returns Dict[int, PerceptionObjectModel] - The POM history. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pom_history = pipeline.get_pom_history()","title":"get_pom_history"},{"location":"pipeline/#run_perceptor","text":"def run_perceptor(perceptor: Perceptor, input_data: Any, multi: bool = False) -> Any Runs the Perceptor. Arguments perceptor ( Perceptor ): The Perceptor to be run. input_data ( Any ): The input data. multi ( bool ): Whether or not to run the perceptor for each item in input data. Defaults to False . Returns Any : The result of running the Perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> result = pipeline.run_perceptor(perceptor=Perceptor(), input_data=None, multi=True)","title":"run_perceptor"},{"location":"pipeline/#get_graph","text":"def get_graph() -> Any Gets the graph of the perceptors. Returns Any : The graph of the perceptors. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> graph = pipeline.get_graph()","title":"get_graph"},{"location":"pipeline/#get_all_performance_metrics","text":"def get_all_performance_metrics() -> Dict[str, Any] Gets the performance metrics of the pipeline. Returns Dict[str, Any] - The performance metrics of the pipeline. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_all_performance_metrics()","title":"get_all_performance_metrics"},{"location":"pipeline/#get_pulse_performance_metrics","text":"def get_pulse_performance_metrics( pulse_number: Union[int, None] = None) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific pulse. Arguments pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific pulse. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_pulse_performance_metrics(pulse_number=1)","title":"get_pulse_performance_metrics"},{"location":"pipeline/#get_perceptor_performance_metrics","text":"def get_perceptor_performance_metrics(name: str, pulse_number: Union[int, None] = None ) -> Dict[str, Any] Gets the performance metrics of the pipeline for specific perceptor. Arguments name ( str ): The name of the perceptor. pulse_number ( int ): The pulse number of the pulse. Defaults to current pulse. Defaults to None . Returns Dict[str, Any] - The performance metrics of the pipeline for specific perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> metrics = pipeline.get_perceptor_performance_metrics(name=\"perceptor_name\", ... pulse_number=1)","title":"get_perceptor_performance_metrics"},{"location":"pipeline/#set_perceptor_config","text":"def set_perceptor_config(perceptor_name: str, name: str, value: Any) -> None Sets the config of the pipeline. Arguments perceptor_name ( str ): The name of the perceptor. name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_perceptor_config(perceptor_name=\"perceptor_name\", ... name=\"config_name\", ... value=1)","title":"set_perceptor_config"},{"location":"pipeline/#get_perceptor_config","text":"def get_perceptor_config(perceptor_name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the perceptor. Arguments perceptor_name ( str ): The name of the perceptor. Returns Dict[str, Tuple[Any, Config]] - The config of the perceptor. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_perceptor_config(perceptor_name=\"perceptor_name\")","title":"get_perceptor_config"},{"location":"pipeline/#set_output_stream_config","text":"def set_output_stream_config(name: str, config_name: str, value: Any) -> None Sets the config of the output stream. Arguments name ( str ): The name of the output stream. config_name ( str ): The name of the config. value ( Any ): The value of the config. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> pipeline.set_output_stream_config(name=\"output_stream_name\", ... config_name=\"config_name\", ... value=1)","title":"set_output_stream_config"},{"location":"pipeline/#get_output_stream_config","text":"def get_output_stream_config(name: str) -> Dict[str, Tuple[Any, Config]] Gets the config of the output stream. Arguments name ( str ): The name of the output stream. Returns Dict[str, Tuple[Any, Config]] - The config of the output stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.pipeline import Pipeline >>> camera = CameraStream(video_device=\"/dev/video0\") >>> pipeline = Pipeline(input_stream=camera) >>> config = pipeline.get_output_stream_config(name=\"output_stream_name\")","title":"get_output_stream_config"},{"location":"serializable/","text":"darcyai.serializable Serializable Objects class Serializable() Base class for all serializable objects. serialize def serialize() -> Dict[str, Any] Serializes the object into a dictionary. Returns Dict[str, Any] - The serialized object.","title":"Serializable"},{"location":"serializable/#darcyaiserializable","text":"","title":"darcyai.serializable"},{"location":"serializable/#serializable-objects","text":"class Serializable() Base class for all serializable objects.","title":"Serializable Objects"},{"location":"serializable/#serialize","text":"def serialize() -> Dict[str, Any] Serializes the object into a dictionary. Returns Dict[str, Any] - The serialized object.","title":"serialize"},{"location":"streamdata/","text":"darcyai.stream_data StreamData Objects class StreamData(Serializable) Class to hold data from a stream. Arguments data ( Any ): The data to be stored. timestamp ( int ): The timestamp of the data. serialize def serialize() -> dict Serializes the data. Returns dict : The serialized data.","title":"StreamData"},{"location":"streamdata/#darcyaistream_data","text":"","title":"darcyai.stream_data"},{"location":"streamdata/#streamdata-objects","text":"class StreamData(Serializable) Class to hold data from a stream. Arguments data ( Any ): The data to be stored. timestamp ( int ): The timestamp of the data.","title":"StreamData Objects"},{"location":"streamdata/#serialize","text":"def serialize() -> dict Serializes the data. Returns dict : The serialized data.","title":"serialize"},{"location":"terminology/","text":"Darcy AI platform terminology guide Before you begin building with the Darcy AI SDK or testing its functionality, you need to understand the terminology and basic architecture. This document will give you an understanding quickly. Engine The Darcy AI engine is the part that runs the AI computations and manages the system resources such as memory and CPU threads. It can be considered the core \u201cbackend code\u201d of the Darcy AI SDK. As a developer, you do not need to interact with the engine directly. You only need to use the provided interfaces in the API as described in the documentation. Pipeline Every Darcy AI application is allowed one pipeline. A Darcy AI pipeline is the sequenced set of AI processes that does the real work in the application. The Darcy AI pipeline code object is one of the main objects that you will interact with as a developer. It contains many important methods and the AI processing starts when the \u201crun()\u201d method is called. Perceptor A Darcy AI perceptor is a code module that integrates raw AI processing with CPU code to make an easy-to-use semantic interface for the underlying AI output. Perceptors are built by developers who understand AI programming but are used by AI application developers who want to leverage the perceptor abilities. This frees AI application developers from needing to become AI experts and opens a perceptor library ecosystem. Perception Object Model (POM) Similar to the Document Object Model (DOM) that is found in web browsers, the Perception Object Model (POM) is a data tree structure found in Darcy AI applications. The POM is the place where the outputs of each pipeline step are stored. The POM is available to Darcy AI application developers at each pipeline step, when a whole pipeline cycle has been completed, and at any point when the developer desires to interact with it. The POM also contains a history of all AI raw inputs and processing results. Input Stream A Darcy AI input stream is the source data that is used for AI processing. Because Darcy\u2019s \u201csenses\u201d can be expanded to include any source of data, an input stream code object is used to encapsulate the processing that is done to prepare incoming data for AI workloads. An example of an input stream code library is one that captures the frames of video from a camera and also merges the thermal camera data with each frame, even though the two cameras provide data at different rates. An input stream is attached to a pipeline by you, the Darcy AI application developer. Output Stream A Darcy AI output stream is a code library that receives the data from the pipeline processing and produces a useful output, such as a video display or a CSV file. Many output streams can be attached to a single pipeline by you, the Darcy AI application developer. Callback For every step in the Darcy AI application processing, work is needed to format and produce business value from incoming data and outgoing data. The way that Darcy allows developers to do this work is to have their code processed by Darcy when the time is right. This is called a \u201ccallback\u201d and it is a well-known pattern of software development in JavaScript and other languages. By using callbacks, developers can focus on just the pieces of code that relate to their actual application and know that Darcy will run their code for them. Frame, Cycle, or Pulse Every complete trip through a Darcy AI pipeline is called a frame. It can also be called a cycle or a pulse. Initialization In order to allow Darcy to start doing AI processing, some foundational settings must be chosen and some basic requirements must be met, such as providing an input stream. Then the Darcy AI pipeline needs to be started so the application can run. These steps are called the Darcy AI initialization and they must be performed by the developer in every application. Docker Base Image There are many software packages and libraries that Darcy AI applications need in order to build and run properly. Asking you, the developer, to know and understand these dependencies would slow you down and cause you unecessary complexity. To circumvent this problem, the required software is bundled ahead of time in easy-to-use base images that are Docker containers. Because they are already Docker containers, you can make your application Docker containers easily by starting from one of the provided base container images. Performance Metrics Darcy tracks her own performance when doing AI processing. Each trip through the pipeline steps is measured, along with the individual pipeline steps. Darcy AI application developers can request this performance data in their application, which allows for benchmarking, profiling, and innovative displays that show how fast each part of Darcy\u2019s work is being done. AI Model The actual AI neural network processing is done using AI models. An AI model is a stored image of a neural net that was built during an AI training or retraining process. Most developers use AI models that already exist and were created by someone else. Darcy AI perceptors contain AI models and make them easier to use. Most Darcy AI application developers do not need to use AI models directly because of the perceptor architecture.","title":"Terminology"},{"location":"terminology/#darcy-ai-platform-terminology-guide","text":"Before you begin building with the Darcy AI SDK or testing its functionality, you need to understand the terminology and basic architecture. This document will give you an understanding quickly.","title":"Darcy AI platform terminology guide"},{"location":"terminology/#engine","text":"The Darcy AI engine is the part that runs the AI computations and manages the system resources such as memory and CPU threads. It can be considered the core \u201cbackend code\u201d of the Darcy AI SDK. As a developer, you do not need to interact with the engine directly. You only need to use the provided interfaces in the API as described in the documentation.","title":"Engine"},{"location":"terminology/#pipeline","text":"Every Darcy AI application is allowed one pipeline. A Darcy AI pipeline is the sequenced set of AI processes that does the real work in the application. The Darcy AI pipeline code object is one of the main objects that you will interact with as a developer. It contains many important methods and the AI processing starts when the \u201crun()\u201d method is called.","title":"Pipeline"},{"location":"terminology/#perceptor","text":"A Darcy AI perceptor is a code module that integrates raw AI processing with CPU code to make an easy-to-use semantic interface for the underlying AI output. Perceptors are built by developers who understand AI programming but are used by AI application developers who want to leverage the perceptor abilities. This frees AI application developers from needing to become AI experts and opens a perceptor library ecosystem.","title":"Perceptor"},{"location":"terminology/#perception-object-model-pom","text":"Similar to the Document Object Model (DOM) that is found in web browsers, the Perception Object Model (POM) is a data tree structure found in Darcy AI applications. The POM is the place where the outputs of each pipeline step are stored. The POM is available to Darcy AI application developers at each pipeline step, when a whole pipeline cycle has been completed, and at any point when the developer desires to interact with it. The POM also contains a history of all AI raw inputs and processing results.","title":"Perception Object Model (POM)"},{"location":"terminology/#input-stream","text":"A Darcy AI input stream is the source data that is used for AI processing. Because Darcy\u2019s \u201csenses\u201d can be expanded to include any source of data, an input stream code object is used to encapsulate the processing that is done to prepare incoming data for AI workloads. An example of an input stream code library is one that captures the frames of video from a camera and also merges the thermal camera data with each frame, even though the two cameras provide data at different rates. An input stream is attached to a pipeline by you, the Darcy AI application developer.","title":"Input Stream"},{"location":"terminology/#output-stream","text":"A Darcy AI output stream is a code library that receives the data from the pipeline processing and produces a useful output, such as a video display or a CSV file. Many output streams can be attached to a single pipeline by you, the Darcy AI application developer.","title":"Output Stream"},{"location":"terminology/#callback","text":"For every step in the Darcy AI application processing, work is needed to format and produce business value from incoming data and outgoing data. The way that Darcy allows developers to do this work is to have their code processed by Darcy when the time is right. This is called a \u201ccallback\u201d and it is a well-known pattern of software development in JavaScript and other languages. By using callbacks, developers can focus on just the pieces of code that relate to their actual application and know that Darcy will run their code for them.","title":"Callback"},{"location":"terminology/#frame-cycle-or-pulse","text":"Every complete trip through a Darcy AI pipeline is called a frame. It can also be called a cycle or a pulse.","title":"Frame, Cycle, or Pulse"},{"location":"terminology/#initialization","text":"In order to allow Darcy to start doing AI processing, some foundational settings must be chosen and some basic requirements must be met, such as providing an input stream. Then the Darcy AI pipeline needs to be started so the application can run. These steps are called the Darcy AI initialization and they must be performed by the developer in every application.","title":"Initialization"},{"location":"terminology/#docker-base-image","text":"There are many software packages and libraries that Darcy AI applications need in order to build and run properly. Asking you, the developer, to know and understand these dependencies would slow you down and cause you unecessary complexity. To circumvent this problem, the required software is bundled ahead of time in easy-to-use base images that are Docker containers. Because they are already Docker containers, you can make your application Docker containers easily by starting from one of the provided base container images.","title":"Docker Base Image"},{"location":"terminology/#performance-metrics","text":"Darcy tracks her own performance when doing AI processing. Each trip through the pipeline steps is measured, along with the individual pipeline steps. Darcy AI application developers can request this performance data in their application, which allows for benchmarking, profiling, and innovative displays that show how fast each part of Darcy\u2019s work is being done.","title":"Performance Metrics"},{"location":"terminology/#ai-model","text":"The actual AI neural network processing is done using AI models. An AI model is a stored image of a neural net that was built during an AI training or retraining process. Most developers use AI models that already exist and were created by someone else. Darcy AI perceptors contain AI models and make them easier to use. Most Darcy AI application developers do not need to use AI models directly because of the perceptor architecture.","title":"AI Model"},{"location":"coral-perceptors/imageclassificationperceptor/","text":"darcyai.perceptor.coral.image_classification_perceptor ImageClassificationPerceptor Objects class ImageClassificationPerceptor(CoralPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments : threshold float - The threshold for object detection. top_k int - The number of top predictions to return. labels_file str - The path to the labels file. labels dict - A dictionary of labels. mean float - The mean of the image. std float - The standard deviation of the image. **kwargs - Keyword arguments to pass to Perceptor. run def run(input_data: Any, config: ConfigRegistry = None) -> (List[Any], List[str]) Runs the image classification model. Arguments : input_data Any - The input data to run the model on. config ConfigRegistry - The configuration for the perceptor. Returns : (list[Any], list(str)): A tuple containing the detected classes and the labels. load def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments : accelerator_idx int - The index of the Edge TPU to use. Returns : None","title":"ImageClassificationPerceptor"},{"location":"coral-perceptors/imageclassificationperceptor/#darcyaiperceptorcoralimage_classification_perceptor","text":"","title":"darcyai.perceptor.coral.image_classification_perceptor"},{"location":"coral-perceptors/imageclassificationperceptor/#imageclassificationperceptor-objects","text":"class ImageClassificationPerceptor(CoralPerceptorBase) ImageClassificationPerceptor is a class that implements the Perceptor interface for image classification. Arguments : threshold float - The threshold for object detection. top_k int - The number of top predictions to return. labels_file str - The path to the labels file. labels dict - A dictionary of labels. mean float - The mean of the image. std float - The standard deviation of the image. **kwargs - Keyword arguments to pass to Perceptor.","title":"ImageClassificationPerceptor Objects"},{"location":"coral-perceptors/imageclassificationperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> (List[Any], List[str]) Runs the image classification model. Arguments : input_data Any - The input data to run the model on. config ConfigRegistry - The configuration for the perceptor. Returns : (list[Any], list(str)): A tuple containing the detected classes and the labels.","title":"run"},{"location":"coral-perceptors/imageclassificationperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the image classification model. Arguments : accelerator_idx int - The index of the Edge TPU to use. Returns : None","title":"load"},{"location":"coral-perceptors/objectdetectionperceptor/","text":"darcyai.perceptor.coral.object_detection_perceptor ObjectDetectionPerceptor Objects class ObjectDetectionPerceptor(CoralPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments : threshold float - The threshold for object detection. labels_file str - The path to the labels file. **kwargs - Keyword arguments to pass to Perceptor. run def run(input_data: Any, config: ConfigRegistry = None) -> (List[Any], List[str]) Runs the object detection model on the input data. Arguments : input_data Any - The input data to run the model on. config ConfigRegistry - The configuration for the Perceptor. Returns : (list[Any], list(str)): A tuple containing the detected objects and the labels. load def load(accelerator_idx: [int, None]) -> None Loads the object detection model. Arguments : accelerator_idx int - The index of the Edge TPU to use. Returns : None","title":"ObjectDetectionPerceptor"},{"location":"coral-perceptors/objectdetectionperceptor/#darcyaiperceptorcoralobject_detection_perceptor","text":"","title":"darcyai.perceptor.coral.object_detection_perceptor"},{"location":"coral-perceptors/objectdetectionperceptor/#objectdetectionperceptor-objects","text":"class ObjectDetectionPerceptor(CoralPerceptorBase) ObjectDetectionPerceptor is a class that implements the Perceptor interface for object detection. Arguments : threshold float - The threshold for object detection. labels_file str - The path to the labels file. **kwargs - Keyword arguments to pass to Perceptor.","title":"ObjectDetectionPerceptor Objects"},{"location":"coral-perceptors/objectdetectionperceptor/#run","text":"def run(input_data: Any, config: ConfigRegistry = None) -> (List[Any], List[str]) Runs the object detection model on the input data. Arguments : input_data Any - The input data to run the model on. config ConfigRegistry - The configuration for the Perceptor. Returns : (list[Any], list(str)): A tuple containing the detected objects and the labels.","title":"run"},{"location":"coral-perceptors/objectdetectionperceptor/#load","text":"def load(accelerator_idx: [int, None]) -> None Loads the object detection model. Arguments : accelerator_idx int - The index of the Edge TPU to use. Returns : None","title":"load"},{"location":"coral-perceptors/peopleperceptor/","text":"darcyai.perceptor.coral.people_perceptor KeypointType Objects class KeypointType(enum.IntEnum) Pose kepoints. PoseEngine Objects class PoseEngine() Engine used for pose tasks. __init__ def __init__(model_path, mirror=False, arch=os.uname().machine) Creates a PoseEngine with given model. Arguments : model_path - String, path to TF-Lite Flatbuffer file. mirror - Flip keypoints horizontally. Raises : ValueError - An error occurred when model output is invalid. run_inference def run_inference(input_data) Run inference using the zero copy feature from pycoral and returns inference time in ms. DetectPosesInImage def DetectPosesInImage(img) Detects poses in a given image. For ideal results make sure the image fed to this function is close to the expected input size - it is the caller's responsibility to resize the image accordingly. Arguments : img - numpy array containing image get_input_tensor_shape def get_input_tensor_shape() Returns input tensor shape. get_output_tensor def get_output_tensor(idx) Returns output tensor view. ParseOutput def ParseOutput() Parses interpreter output tensors and returns decoded poses.","title":"PeoplePerceptor"},{"location":"coral-perceptors/peopleperceptor/#darcyaiperceptorcoralpeople_perceptor","text":"","title":"darcyai.perceptor.coral.people_perceptor"},{"location":"coral-perceptors/peopleperceptor/#keypointtype-objects","text":"class KeypointType(enum.IntEnum) Pose kepoints.","title":"KeypointType Objects"},{"location":"coral-perceptors/peopleperceptor/#poseengine-objects","text":"class PoseEngine() Engine used for pose tasks.","title":"PoseEngine Objects"},{"location":"coral-perceptors/peopleperceptor/#__init__","text":"def __init__(model_path, mirror=False, arch=os.uname().machine) Creates a PoseEngine with given model. Arguments : model_path - String, path to TF-Lite Flatbuffer file. mirror - Flip keypoints horizontally. Raises : ValueError - An error occurred when model output is invalid.","title":"__init__"},{"location":"coral-perceptors/peopleperceptor/#run_inference","text":"def run_inference(input_data) Run inference using the zero copy feature from pycoral and returns inference time in ms.","title":"run_inference"},{"location":"coral-perceptors/peopleperceptor/#detectposesinimage","text":"def DetectPosesInImage(img) Detects poses in a given image. For ideal results make sure the image fed to this function is close to the expected input size - it is the caller's responsibility to resize the image accordingly. Arguments : img - numpy array containing image","title":"DetectPosesInImage"},{"location":"coral-perceptors/peopleperceptor/#get_input_tensor_shape","text":"def get_input_tensor_shape() Returns input tensor shape.","title":"get_input_tensor_shape"},{"location":"coral-perceptors/peopleperceptor/#get_output_tensor","text":"def get_output_tensor(idx) Returns output tensor view.","title":"get_output_tensor"},{"location":"coral-perceptors/peopleperceptor/#parseoutput","text":"def ParseOutput() Parses interpreter output tensors and returns decoded poses.","title":"ParseOutput"},{"location":"coral-perceptors/peoplepom/","text":"darcyai.perceptor.coral.people_perceptor_pom","title":"PeoplePOM"},{"location":"coral-perceptors/peoplepom/#darcyaiperceptorcoralpeople_perceptor_pom","text":"","title":"darcyai.perceptor.coral.people_perceptor_pom"},{"location":"input-streams/camerastream/","text":"darcyai.input.camera_stream CameraStream Objects class CameraStream(InputStream) An input stream that gets frames from camera. Arguments use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. Defaults to False . video_device ( str ): The video device to use. Defaults to None . video_width ( int ): The width of the video frames. Defaults to 640 . video_height ( int ): The height of the video frames. Defaults to 480 . flip_frames ( bool ): Whether or not to flip the video frames. Defaults to False . fps ( int ): The frames per second to stream. Defaults to 30 . Examples >>> from darcyai.input.camera_stream import CameraStream >>> pi_camera = CameraStream(use_pi_camera=True) >>> usb_camera = CameraStream(video_device=\"/dev/video0\") stop def stop() -> None Stops the video stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stop() stream def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stream() get_video_inputs @staticmethod def get_video_inputs() Gets the available video inputs. Returns int[] : A list of strings. Examples >>> from darcyai.input.camera_stream import CameraStream >>> CameraStream.get_video_inputs()","title":"CameraStream"},{"location":"input-streams/camerastream/#darcyaiinputcamera_stream","text":"","title":"darcyai.input.camera_stream"},{"location":"input-streams/camerastream/#camerastream-objects","text":"class CameraStream(InputStream) An input stream that gets frames from camera. Arguments use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. Defaults to False . video_device ( str ): The video device to use. Defaults to None . video_width ( int ): The width of the video frames. Defaults to 640 . video_height ( int ): The height of the video frames. Defaults to 480 . flip_frames ( bool ): Whether or not to flip the video frames. Defaults to False . fps ( int ): The frames per second to stream. Defaults to 30 . Examples >>> from darcyai.input.camera_stream import CameraStream >>> pi_camera = CameraStream(use_pi_camera=True) >>> usb_camera = CameraStream(video_device=\"/dev/video0\")","title":"CameraStream Objects"},{"location":"input-streams/camerastream/#stop","text":"def stop() -> None Stops the video stream. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stop()","title":"stop"},{"location":"input-streams/camerastream/#stream","text":"def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.camera_stream import CameraStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> usb_camera.stream()","title":"stream"},{"location":"input-streams/camerastream/#get_video_inputs","text":"@staticmethod def get_video_inputs() Gets the available video inputs. Returns int[] : A list of strings. Examples >>> from darcyai.input.camera_stream import CameraStream >>> CameraStream.get_video_inputs()","title":"get_video_inputs"},{"location":"input-streams/inputmultistream/","text":"darcyai.input.input_multi_stream InputMultiStream Objects class InputMultiStream() A class that represents a collection of input streams. Arguments aggregator ( Callable[[None], StreamData] ): a function that takes a list of data and returns a single data point callback ( Callable[[StreamData], None] ): a function that gets called when data is received from a stream Examples >>> from darcyai.input.input_multi_stream import InputMultiStream >>> from darcyai.stream_data import StreamData >>> def aggregator(): ... return StreamData(\"data\", 1234567890) >>> def callback(data: StreamData): ... print(data.data, data.timestamp) >>> input_stream = InputMultiStream(aggregator, callback) remove_stream def remove_stream(name: str) -> None Removes a stream from the collection. Arguments name ( str ): the name of the stream to remove Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.remove_stream(\"usb_camera\") get_stream def get_stream(name: str) -> InputStream Gets a stream from the collection. Arguments name ( str ): the name of the stream to get Returns InputStream : the stream with the given name Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.get_stream(\"usb_camera\") add_stream def add_stream(name: str, stream: InputStream) -> None Adds a stream to the collection. Arguments name ( str ): the name of the stream to add stream ( InputStream ): the stream to add Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) stream def stream() -> Iterable[StreamData] Starts streaming data from all streams in the collection. Returns Iterable[StreamData] : an iterable of data from all streams Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stream() stop def stop() -> None Stops streaming data from all streams in the collection. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stop()","title":"InputMultiStream"},{"location":"input-streams/inputmultistream/#darcyaiinputinput_multi_stream","text":"","title":"darcyai.input.input_multi_stream"},{"location":"input-streams/inputmultistream/#inputmultistream-objects","text":"class InputMultiStream() A class that represents a collection of input streams. Arguments aggregator ( Callable[[None], StreamData] ): a function that takes a list of data and returns a single data point callback ( Callable[[StreamData], None] ): a function that gets called when data is received from a stream Examples >>> from darcyai.input.input_multi_stream import InputMultiStream >>> from darcyai.stream_data import StreamData >>> def aggregator(): ... return StreamData(\"data\", 1234567890) >>> def callback(data: StreamData): ... print(data.data, data.timestamp) >>> input_stream = InputMultiStream(aggregator, callback)","title":"InputMultiStream Objects"},{"location":"input-streams/inputmultistream/#remove_stream","text":"def remove_stream(name: str) -> None Removes a stream from the collection. Arguments name ( str ): the name of the stream to remove Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.remove_stream(\"usb_camera\")","title":"remove_stream"},{"location":"input-streams/inputmultistream/#get_stream","text":"def get_stream(name: str) -> InputStream Gets a stream from the collection. Arguments name ( str ): the name of the stream to get Returns InputStream : the stream with the given name Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.get_stream(\"usb_camera\")","title":"get_stream"},{"location":"input-streams/inputmultistream/#add_stream","text":"def add_stream(name: str, stream: InputStream) -> None Adds a stream to the collection. Arguments name ( str ): the name of the stream to add stream ( InputStream ): the stream to add Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera)","title":"add_stream"},{"location":"input-streams/inputmultistream/#stream","text":"def stream() -> Iterable[StreamData] Starts streaming data from all streams in the collection. Returns Iterable[StreamData] : an iterable of data from all streams Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stream()","title":"stream"},{"location":"input-streams/inputmultistream/#stop","text":"def stop() -> None Stops streaming data from all streams in the collection. Examples >>> from darcyai.input.camera_stream import CameraStream >>> from darcyai.input.input_multi_stream import InputMultiStream >>> usb_camera = CameraStream(video_device=\"/dev/video0\") >>> input_stream = InputMultiStream(aggregator, callback) >>> input_stream.add_stream(\"usb_camera\", usb_camera) >>> input_stream.stop()","title":"stop"},{"location":"input-streams/inputstream/","text":"darcyai.input.input_stream InputStream Objects class InputStream() Base class for reading input from a stream. Examples >>> from darcyai.input.input_stream import InputStream >>> from darcyai.stream_data import StreamData >>> class MyInputStream(InputStream): ... def stream(self): ... yield StreamData(\"data\", 1234567890) >>> def stop(self): ... pass stream def stream() -> Iterable[StreamData] Returns a generator that yields a stream of input. Returns A generator that yields a stream of input. stop def stop() -> None Stops the stream.","title":"InputStream"},{"location":"input-streams/inputstream/#darcyaiinputinput_stream","text":"","title":"darcyai.input.input_stream"},{"location":"input-streams/inputstream/#inputstream-objects","text":"class InputStream() Base class for reading input from a stream. Examples >>> from darcyai.input.input_stream import InputStream >>> from darcyai.stream_data import StreamData >>> class MyInputStream(InputStream): ... def stream(self): ... yield StreamData(\"data\", 1234567890) >>> def stop(self): ... pass","title":"InputStream Objects"},{"location":"input-streams/inputstream/#stream","text":"def stream() -> Iterable[StreamData] Returns a generator that yields a stream of input. Returns A generator that yields a stream of input.","title":"stream"},{"location":"input-streams/inputstream/#stop","text":"def stop() -> None Stops the stream.","title":"stop"},{"location":"input-streams/videofilestream/","text":"darcyai.input.video_file_stream VideoFileStream Objects class VideoFileStream(InputStream) An input stream that reads frames from a video file. Arguments file_name ( str ): The name of the video file to stream. use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. loop ( bool ): Whether or not to loop the video. Defaults to True . process_all_frames ( bool ): Whether or not to process all frames. Defaults to True . Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) stop def stop() -> None Stops the video stream. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stop() stream def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stream()","title":"VideoFileStream"},{"location":"input-streams/videofilestream/#darcyaiinputvideo_file_stream","text":"","title":"darcyai.input.video_file_stream"},{"location":"input-streams/videofilestream/#videofilestream-objects","text":"class VideoFileStream(InputStream) An input stream that reads frames from a video file. Arguments file_name ( str ): The name of the video file to stream. use_pi_camera ( bool ): Whether or not to use the Raspberry Pi camera. loop ( bool ): Whether or not to loop the video. Defaults to True . process_all_frames ( bool ): Whether or not to process all frames. Defaults to True . Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True)","title":"VideoFileStream Objects"},{"location":"input-streams/videofilestream/#stop","text":"def stop() -> None Stops the video stream. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stop()","title":"stop"},{"location":"input-streams/videofilestream/#stream","text":"def stream() -> Iterable[VideoStreamData] Streams the video frames. Returns An iterable of VideoStreamData objects. Examples >>> from darcyai.input.video_file_stream import VideoFileStream >>> video_file_stream = VideoFileStream(file_name=\"video.mp4\", loop=True, process_all_frames=True) >>> video_file_stream.stream()","title":"stream"},{"location":"input-streams/videostreamdata/","text":"darcyai.input.video_stream_data VideoStreamData Objects class VideoStreamData(StreamData) StreamData representation of video frames serialize def serialize() -> dict Serialize the data to a dict Returns dict : serialized data Examples >>> from darcyai.input.video_stream_data import VideoStreamData >>> data = VideoStreamData(frame, timestamp) >>> data.serialize() { \"frame\": \"base64 encoded frame in jpeg format\", \"timestamp\": 1638482728 }","title":"VideoStreamData"},{"location":"input-streams/videostreamdata/#darcyaiinputvideo_stream_data","text":"","title":"darcyai.input.video_stream_data"},{"location":"input-streams/videostreamdata/#videostreamdata-objects","text":"class VideoStreamData(StreamData) StreamData representation of video frames","title":"VideoStreamData Objects"},{"location":"input-streams/videostreamdata/#serialize","text":"def serialize() -> dict Serialize the data to a dict Returns dict : serialized data Examples >>> from darcyai.input.video_stream_data import VideoStreamData >>> data = VideoStreamData(frame, timestamp) >>> data.serialize() { \"frame\": \"base64 encoded frame in jpeg format\", \"timestamp\": 1638482728 }","title":"serialize"},{"location":"output-streams/csvoutputstream/","text":"darcyai.output.csv_output_stream CSVOutputStream Objects class CSVOutputStream(OutputStream) OutputStream implementation that writes to a CSV file. Arguments file_path ( str ): The path to the CSV file to write to. delimiter ( str ): The delimiter to use in the CSV file. Defaults to , . quotechar ( str ): The quote character to use in the CSV file. Defaults to | . buffer_size ( int ): The size of the buffer to use when writing to theCSV file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) write def write(data: list) -> None Writes the given data to the CSV file. Arguments data ( list ): The data to write to the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.write([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]]) close def close() -> None Closes the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.close()","title":"CSVOutputStream"},{"location":"output-streams/csvoutputstream/#darcyaioutputcsv_output_stream","text":"","title":"darcyai.output.csv_output_stream"},{"location":"output-streams/csvoutputstream/#csvoutputstream-objects","text":"class CSVOutputStream(OutputStream) OutputStream implementation that writes to a CSV file. Arguments file_path ( str ): The path to the CSV file to write to. delimiter ( str ): The delimiter to use in the CSV file. Defaults to , . quotechar ( str ): The quote character to use in the CSV file. Defaults to | . buffer_size ( int ): The size of the buffer to use when writing to theCSV file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0)","title":"CSVOutputStream Objects"},{"location":"output-streams/csvoutputstream/#write","text":"def write(data: list) -> None Writes the given data to the CSV file. Arguments data ( list ): The data to write to the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.write([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]])","title":"write"},{"location":"output-streams/csvoutputstream/#close","text":"def close() -> None Closes the CSV file. Examples >>> from darcyai.output.csv_output_stream import CSVOutputStream >>> csv_output_stream = CSVOutputStream(file_path=\"output.csv\", delimiter=\",\", quotechar=\"|\", buffer_size=1024*1024, flush_interval=0) >>> csv_output_stream.close()","title":"close"},{"location":"output-streams/jsonoutputstream/","text":"darcyai.output.json_output_stream JSONOutputStream Objects class JSONOutputStream(OutputStream) OutputStream implementation that writes to a JSON file. Arguments file_path ( str ): The path to the JSON file to write to. buffer_size ( int ): The size of the buffer to use when writing to the JSON file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) write def write(data: dict) -> None Writes the given data to the JSON file. Arguments data ( dict ): The data to write to the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.write({\"key\": \"value\"}) close def close() -> None Closes the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.close()","title":"JSONOutputStream"},{"location":"output-streams/jsonoutputstream/#darcyaioutputjson_output_stream","text":"","title":"darcyai.output.json_output_stream"},{"location":"output-streams/jsonoutputstream/#jsonoutputstream-objects","text":"class JSONOutputStream(OutputStream) OutputStream implementation that writes to a JSON file. Arguments file_path ( str ): The path to the JSON file to write to. buffer_size ( int ): The size of the buffer to use when writing to the JSON file. Defaults to 0 . flush_interval ( int ): The number of seconds before flushing the buffer to disk. Defaults to 0 . Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0)","title":"JSONOutputStream Objects"},{"location":"output-streams/jsonoutputstream/#write","text":"def write(data: dict) -> None Writes the given data to the JSON file. Arguments data ( dict ): The data to write to the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.write({\"key\": \"value\"})","title":"write"},{"location":"output-streams/jsonoutputstream/#close","text":"def close() -> None Closes the JSON file. Examples >>> from darcyai.output.json_output_stream import JSONOutputStream >>> json_output_stream = JSONOutputStream(file_path=\"output.csv\", buffer_size=0) >>> json_output_stream.close()","title":"close"},{"location":"output-streams/livefeedstream/","text":"darcyai.output.live_feed_stream LiveFeedStream Objects class LiveFeedStream(OutputStream) An output stream that streams the frames. Arguments path ( str ): Path to host the live stream. flask_app ( Flask ): Flask app to host the live stream. Defaults to None . port ( int ): Port to host the live stream. Defaults to None . host ( str ): Host to host the live stream. Defaults to None . fps ( int ): Frames per second to stream. Defaults to 20 . quality ( int ): Quality of the JPEG encoding. Defaults to 100 . Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) write def write(data: Any) -> Any Write a frame to the stream. Arguments data ( Any ): Frame to write. Returns Any : The annotated frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.write(frame) get_fps def get_fps() -> int Get the frames per second. Returns int : Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_fps() set_fps def set_fps(fps: int) -> None Set the frames per second. Arguments fps ( int ): Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_fps(30) get_quality def get_quality() -> int Get the quality of the JPEG encoding. Returns int : Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_quality() set_quality def set_quality(quality: int) -> None Set the quality of the JPEG encoding. Arguments quality ( int ): Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_quality(50) close def close() -> None Close the stream. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.close() get_latest_frame def get_latest_frame() -> Any Returns the latest frame in JPEG format. Returns Any : Latest frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_latest_frame()","title":"LiveFeedStream"},{"location":"output-streams/livefeedstream/#darcyaioutputlive_feed_stream","text":"","title":"darcyai.output.live_feed_stream"},{"location":"output-streams/livefeedstream/#livefeedstream-objects","text":"class LiveFeedStream(OutputStream) An output stream that streams the frames. Arguments path ( str ): Path to host the live stream. flask_app ( Flask ): Flask app to host the live stream. Defaults to None . port ( int ): Port to host the live stream. Defaults to None . host ( str ): Host to host the live stream. Defaults to None . fps ( int ): Frames per second to stream. Defaults to 20 . quality ( int ): Quality of the JPEG encoding. Defaults to 100 . Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100)","title":"LiveFeedStream Objects"},{"location":"output-streams/livefeedstream/#write","text":"def write(data: Any) -> Any Write a frame to the stream. Arguments data ( Any ): Frame to write. Returns Any : The annotated frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.write(frame)","title":"write"},{"location":"output-streams/livefeedstream/#get_fps","text":"def get_fps() -> int Get the frames per second. Returns int : Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_fps()","title":"get_fps"},{"location":"output-streams/livefeedstream/#set_fps","text":"def set_fps(fps: int) -> None Set the frames per second. Arguments fps ( int ): Frames per second. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_fps(30)","title":"set_fps"},{"location":"output-streams/livefeedstream/#get_quality","text":"def get_quality() -> int Get the quality of the JPEG encoding. Returns int : Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_quality()","title":"get_quality"},{"location":"output-streams/livefeedstream/#set_quality","text":"def set_quality(quality: int) -> None Set the quality of the JPEG encoding. Arguments quality ( int ): Quality of the JPEG encoding. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.set_quality(50)","title":"set_quality"},{"location":"output-streams/livefeedstream/#close","text":"def close() -> None Close the stream. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.close()","title":"close"},{"location":"output-streams/livefeedstream/#get_latest_frame","text":"def get_latest_frame() -> Any Returns the latest frame in JPEG format. Returns Any : Latest frame. Examples >>> from darcyai.output.live_feed_stream import LiveFeedStream >>> live_feed_stream = LiveFeedStream(path=\"/live-feed\", >>> port=8080, >>> host=\"0.0.0.0\", >>> fps=20, >>> quality=100) >>> live_feed_stream.get_latest_frame()","title":"get_latest_frame"},{"location":"output-streams/outputstream/","text":"darcyai.output.output_stream OutputStream Objects class OutputStream(Configurable) OutputStream is the base class that is used to write output to a stream. Arguments ignore_none ( bool ): Whether or not to call the endpoint when the data is None. Defaults to True . Examples >>> from darcyai.output.output_stream import OutputStream >>> class MyOutputStream(OutputStream): ... def write(self, data: dict): ... print(data) >>> def close(self): ... pass write def write(data: Any) -> Any Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Any : The data that was written to the output stream. close def close() -> None Closes the output stream. set_config_value def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set. get_config_value def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config. init_config_registry def init_config_registry() Initializes the config registry.","title":"OutputStream"},{"location":"output-streams/outputstream/#darcyaioutputoutput_stream","text":"","title":"darcyai.output.output_stream"},{"location":"output-streams/outputstream/#outputstream-objects","text":"class OutputStream(Configurable) OutputStream is the base class that is used to write output to a stream. Arguments ignore_none ( bool ): Whether or not to call the endpoint when the data is None. Defaults to True . Examples >>> from darcyai.output.output_stream import OutputStream >>> class MyOutputStream(OutputStream): ... def write(self, data: dict): ... print(data) >>> def close(self): ... pass","title":"OutputStream Objects"},{"location":"output-streams/outputstream/#write","text":"def write(data: Any) -> Any Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Any : The data that was written to the output stream.","title":"write"},{"location":"output-streams/outputstream/#close","text":"def close() -> None Closes the output stream.","title":"close"},{"location":"output-streams/outputstream/#set_config_value","text":"def set_config_value(key: str, value: Any) Sets a config value. Arguments key ( str ): The key of the config. value ( Any ): The value to set.","title":"set_config_value"},{"location":"output-streams/outputstream/#get_config_value","text":"def get_config_value(key: str) -> Any Gets a config value. Arguments key ( str ): The key of the config. Returns Any : The value of the config.","title":"get_config_value"},{"location":"output-streams/outputstream/#init_config_registry","text":"def init_config_registry() Initializes the config registry.","title":"init_config_registry"},{"location":"output-streams/restapistream/","text":"darcyai.output.rest_api_stream RestApiStream Objects class RestApiStream(OutputStream) A stream that sends data to a REST API. Arguments url ( str ): The URL of the REST API. method ( str ): The HTTP method to use. Must be one of 'POST', 'PUT', 'PATCH'. Defaults to POST . content_type ( str ): The content type of the data. Must be one of 'json' or 'form'. Defaults to json . headers ( dict ): The headers to send with the request. Defaults to None . Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) write def write(data: Any) -> Response Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Response : The response from the REST API. Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) >>> response = rest_api_stream.write({\"data\": \"some data\"}) close def close() -> None Closes the output stream.","title":"RestApiStream"},{"location":"output-streams/restapistream/#darcyaioutputrest_api_stream","text":"","title":"darcyai.output.rest_api_stream"},{"location":"output-streams/restapistream/#restapistream-objects","text":"class RestApiStream(OutputStream) A stream that sends data to a REST API. Arguments url ( str ): The URL of the REST API. method ( str ): The HTTP method to use. Must be one of 'POST', 'PUT', 'PATCH'. Defaults to POST . content_type ( str ): The content type of the data. Must be one of 'json' or 'form'. Defaults to json . headers ( dict ): The headers to send with the request. Defaults to None . Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"})","title":"RestApiStream Objects"},{"location":"output-streams/restapistream/#write","text":"def write(data: Any) -> Response Processes the data and writes it to the output stream. Arguments data ( Any ): The data to be written to the output stream. Returns Response : The response from the REST API. Examples >>> from darcyai.output.rest_api_stream import RestApiStream >>> rest_api_stream = RestApiStream(url=\"http://localhost:5000/api/v1/data\", method=\"POST\", content_type=\"json\") headers={\"Authorization\": \"Bearer ...\"}) >>> response = rest_api_stream.write({\"data\": \"some data\"})","title":"write"},{"location":"output-streams/restapistream/#close","text":"def close() -> None Closes the output stream.","title":"close"}]}